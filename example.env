# An example .env file

# Trainer args have ROMAE_TRAINER_ at the start:
ROMAE_TRAINER_BASE_LR=5e-4
ROMAE_TRAINER_EPOCHS=5
ROMAE_TRAINER_BATCH_SIZE=32
ROMAE_TRAINER_SAVE_EVERY=500
ROMAE_TRAINER_EVAL_EVERY=500

# The pretraining model has ROMAE_PRETRAIN_ at the start:
ROMAE_PRETRAIN_MASK_RATIO=0.7

# Two underscores are required here because the encoder settings are a
# "sub-setting" of the pretraining model
ROMAE_PRETRAIN_ENCODER_CONFIG__DEPTH=4
ROMAE_PRETRAIN_ENCODER_CONFIG__NHEAD=8
ROMAE_PRETRAIN_ENCODER_CONFIG__D_MODEL=342
# Similar for decoder settings:
ROMAE_PRETRAIN_DECODER_CONFIG__DEPTH=2
ROMAE_PRETRAIN_DECODER_CONFIG__NHEAD=4
ROMAE_PRETRAIN_DECODER_CONFIG__D_MODEL=240

# Configuration for the classifier model has ROMAE_CLASSIFIER_ at the start:
ROMAE_CLASSIFIER_DIM_OUTPUT=1
# Similarly to the pretraining model, the encoder settings require two underscores:
ROMAE_CLASSIFIER_ENCODER_CONFIG__DEPTH=3
ROMAE_CLASSIFIER_ENCODER_CONFIG__NHEAD=8
ROMAE_CLASSIFIER_ENCODER_CONFIG__D_MODEL=342
ROMAE_CLASSIFIER_ENCODER_CONFIG__DROP_PATH_RATE=0.3
